# task_unit_cluster.py
"""
ê³µìœ  ì ì¬í‘œí˜„(shared_z_task_unit.npy)ì„ ì‚¬ìš©í•˜ì—¬ Task Unitì„ HDBSCAN ê³„ì¸µ í´ëŸ¬ìŠ¤í„°ë§
(ëª¨ë¸ ì¬í•™ìŠµ ì—†ìŒ). **ì´ íŒŒì¼ ì•ˆì—ì„œ** HDBSCAN íŒŒë¼ë¯¸í„°(env/sub)ë¥¼ ììœ ë¡­ê²Œ ì¡°ì ˆí•  ìˆ˜ ìˆë„ë¡
ì§ì ‘ HDBSCANì„ í˜¸ì¶œí•˜ì—¬ 2ë‹¨ê³„ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
ì¶œë ¥: task_unit_clustering_results.json
"""
import os
import json
import pickle
import numpy as np
from sklearn.cluster import HDBSCAN


def _save_results_task_unit(cluster_results, out_path):
    # ë©”íƒ€ë°ì´í„° ê³„ì‚°
    total_subclusters = sum(
        len(info["act_des_subclusters"]) for info in cluster_results["env_clusters"].values()
    )
    n_units = sum(info["size"] for info in cluster_results["env_clusters"].values())

    results_with_meta = {
        "metadata": {
            "timestamp": __import__("time").strftime("%Y-%m-%d %H:%M:%S"),
            "total_task_units": n_units,
            "total_env_clusters": len(cluster_results["env_clusters"]),
            "total_subclusters": total_subclusters,
            "clustering_method": "hierarchical_hdbscan_shared_z",
        },
        "results": cluster_results,
    }

    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(results_with_meta, f, ensure_ascii=False, indent=2)


def run_task_unit_clustering(
    tu_shared_path: str = "shared_z_task_unit.npy",
    task_unit_vectors_path: str = "task_unit_vectors_legacy.pkl",  # ğŸ”¥ legacy ë²„ì „ ì‚¬ìš©
    out_path: str = "task_unit_clustering_results.json",
    # ìƒìœ„(ENV) ë‹¨ê³„ HDBSCAN íŒŒë¼ë¯¸í„°
    env_min_cluster_size: int = 3,
    env_min_samples: int = 2,
    env_metric: str = "cosine",
    env_epsilon: float = 0.3,  # cluster_selection_epsilon
    # í•˜ìœ„(ACT+DES) ë‹¨ê³„ HDBSCAN íŒŒë¼ë¯¸í„°
    sub_min_cluster_size: int = 2,
    sub_min_samples: int = 1,
    sub_metric: str = "cosine",
    sub_epsilon: float = 0.2,
):
    if not os.path.exists(tu_shared_path):
        raise FileNotFoundError(f"Missing shared reps: {tu_shared_path}. Run shared_rep.py first.")
    if not os.path.exists(task_unit_vectors_path):
        raise FileNotFoundError(f"Missing vectors: {task_unit_vectors_path}.")

    shared_z = np.load(tu_shared_path)
    with open(task_unit_vectors_path, "rb") as f:
        units = pickle.load(f)

    # ë¼ë²¨ (ì¶œë ¥ìš©)
    env_labels = [u.get("env_text", "unknown") for u in units]
    act_labels = [u.get("act_text", "unknown") for u in units]
    des_labels = [u.get("des_text", "") for u in units]

    # 1ë‹¨ê³„: ENV ì¤‘ì‹¬ ìƒìœ„ í´ëŸ¬ìŠ¤í„°ë§ (shared_z ì§ì ‘ ì‚¬ìš©)
    print("1ï¸âƒ£ ENV ê¸°ë°˜ ìƒìœ„ HDBSCAN...")
    env_clusterer = HDBSCAN(
        min_cluster_size=max(2, env_min_cluster_size),
        min_samples=env_min_samples,
        metric=env_metric,
        cluster_selection_epsilon=env_epsilon,
    )
    env_clusters = env_clusterer.fit_predict(shared_z)

    unique_env = sorted(set(int(c) for c in env_clusters if c != -1))
    n_noise = int(np.sum(env_clusters == -1))
    print(f"   ENV í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(unique_env)}, ë…¸ì´ì¦ˆ: {n_noise}")

    # 2ë‹¨ê³„: ê° ENV í´ëŸ¬ìŠ¤í„° ë‚´ ACT+DES í•˜ìœ„ í´ëŸ¬ìŠ¤í„°ë§ (shared_z ì¬ì‚¬ìš©)
    print("2ï¸âƒ£ ê° ENV í´ëŸ¬ìŠ¤í„° ë‚´ ACT+DES HDBSCAN...")
    cluster_results = {"env_clusters": {}, "task_unit_assignments": {}}

    for env_id in unique_env:
        idx = np.where(env_clusters == env_id)[0]
        if len(idx) < 2:
            continue

        sub_shared = shared_z[idx]
        # ì¶©ë¶„í•˜ë©´ HDBSCAN, ì•„ë‹ˆë©´ ë‹¨ì¼ í´ëŸ¬ìŠ¤í„°(0)
        if len(idx) >= max(2, sub_min_cluster_size):
            sub_clusterer = HDBSCAN(
                min_cluster_size=max(2, sub_min_cluster_size),
                min_samples=sub_min_samples,
                metric=sub_metric,
                cluster_selection_epsilon=sub_epsilon,
            )
            sub_clusters = sub_clusterer.fit_predict(sub_shared)
        else:
            sub_clusters = np.zeros(len(idx), dtype=int)

        cluster_results["env_clusters"][int(env_id)] = {
            "size": int(len(idx)),
            "act_des_subclusters": {},
        }

        for sub_id in sorted(set(int(c) for c in sub_clusters if c != -1)):
            sub_idx = idx[sub_clusters == sub_id]

            # Task Unit ëª©ë¡ êµ¬ì„±
            tu_list = []
            for k in sub_idx:
                unit = units[int(k)]
                tu_list.append({
                    "unique_id": unit.get("unique_id", f"TU_{k:04d}"),
                    "title": unit.get("title", ""),
                    "env_tag": unit.get("env_tag", []),
                    "act_tag": unit.get("act_tag", ""),
                    "env_text": env_labels[k],
                    "act_text": act_labels[k],
                    "des_text": des_labels[k],
                    "file_name": unit.get("file_name", ""),
                })
                cluster_results["task_unit_assignments"][tu_list[-1]["unique_id"]] = {
                    "env_cluster": int(env_id),
                    "act_des_cluster": int(sub_id),
                    "cluster_key": f"{int(env_id)}_{int(sub_id)}",
                }

            cluster_results["env_clusters"][int(env_id)]["act_des_subclusters"][int(sub_id)] = {
                "size": int(len(sub_idx)),
                "task_units": tu_list,
            }

        print(
            f"   ENV[{env_id}] â†’ í•˜ìœ„í´ëŸ¬ìŠ¤í„° {len(cluster_results['env_clusters'][int(env_id)]['act_des_subclusters'])}ê°œ"
        )

    _save_results_task_unit(cluster_results, out_path)
    print(f"âœ… Task Unit clustering saved to {out_path}")


if __name__ == "__main__":
    run_task_unit_clustering()