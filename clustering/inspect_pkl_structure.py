# -*- coding: utf-8 -*-
"""
inspect_pkl_structure.py

PKL íŒŒì¼ì˜ êµ¬ì¡°ì™€ ë‚´ë¶€ ìš”ì†Œë¥¼ ìƒì„¸íˆ ë¶„ì„í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
- íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ë° í¬ê¸° í™•ì¸
- ë°ì´í„° íƒ€ì… ë° êµ¬ì¡° ë¶„ì„
- ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
- í•„ë“œë³„ í†µê³„ ì •ë³´
- ì„ë² ë”© ë²¡í„° ì°¨ì› ë° ë¶„í¬ í™•ì¸

ì‹¤í–‰ ì˜ˆì‹œ:
  python inspect_pkl_structure.py --all
  python inspect_pkl_structure.py --task-units
  python inspect_pkl_structure.py --tasks
  python inspect_pkl_structure.py --sample 3
"""

import os
import pickle
import argparse
import numpy as np
import json
from typing import List, Dict, Any, Union
from collections import Counter


# íŒŒì¼ ê²½ë¡œ ì„¤ì •
TU_VEC_PATH = "task_unit_vectors.pkl"
TASK_VEC_PATH = "task_vectors.pkl"
TU_VEC_BACKUP = "task_unit_vectors_backup.pkl"
TASK_VEC_BACKUP = "task_vectors_backup.pkl"


def _print_separator(title: str, char: str = "=", width: int = 80):
    """êµ¬ë¶„ì„  ì¶œë ¥"""
    print(f"\n{char * width}")
    print(f" {title}")
    print(f"{char * width}")


def _print_subsection(title: str, char: str = "-", width: int = 60):
    """ì†Œì œëª© ì¶œë ¥"""
    print(f"\n{char * width}")
    print(f" {title}")
    print(f"{char * width}")


def _format_bytes(size_bytes: int) -> str:
    """ë°”ì´íŠ¸ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜"""
    if size_bytes == 0:
        return "0 B"
    size_names = ["B", "KB", "MB", "GB"]
    import math
    i = int(math.floor(math.log(size_bytes, 1024)))
    p = math.pow(1024, i)
    s = round(size_bytes / p, 2)
    return f"{s} {size_names[i]}"


def _analyze_vector_field(vectors: List[Dict], field_name: str) -> Dict[str, Any]:
    """ë²¡í„° í•„ë“œì˜ í†µê³„ ë¶„ì„"""
    if not vectors or field_name not in vectors[0]:
        return {"exists": False}
    
    # ì²« ë²ˆì§¸ ë²¡í„°ë¡œ ì°¨ì› í™•ì¸
    first_vec = vectors[0][field_name]
    if not isinstance(first_vec, (list, np.ndarray)):
        return {"exists": False, "error": "Not a vector"}
    
    dimension = len(first_vec)
    
    # ëª¨ë“  ë²¡í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
    try:
        all_vectors = np.array([v[field_name] for v in vectors])
        
        stats = {
            "exists": True,
            "dimension": dimension,
            "count": len(vectors),
            "mean": float(np.mean(all_vectors)),
            "std": float(np.std(all_vectors)),
            "min": float(np.min(all_vectors)),
            "max": float(np.max(all_vectors)),
            "shape": all_vectors.shape
        }
        
        # ì²« ë²ˆì§¸ ë²¡í„°ì˜ ì¼ë¶€ ê°’ë“¤
        stats["sample_values"] = first_vec[:10] if len(first_vec) >= 10 else first_vec
        
        return stats
    except Exception as e:
        return {"exists": True, "error": str(e)}


def _analyze_text_field(vectors: List[Dict], field_name: str) -> Dict[str, Any]:
    """í…ìŠ¤íŠ¸ í•„ë“œì˜ í†µê³„ ë¶„ì„"""
    if not vectors or field_name not in vectors[0]:
        return {"exists": False}
    
    values = [v.get(field_name, "") for v in vectors]
    non_empty = [v for v in values if v and str(v).strip()]
    
    # ê¸¸ì´ í†µê³„
    lengths = [len(str(v)) for v in values]
    
    # ê°€ì¥ í”í•œ ê°’ë“¤
    counter = Counter(values)
    most_common = counter.most_common(5)
    
    return {
        "exists": True,
        "total_count": len(values),
        "non_empty_count": len(non_empty),
        "empty_count": len(values) - len(non_empty),
        "avg_length": np.mean(lengths) if lengths else 0,
        "min_length": min(lengths) if lengths else 0,
        "max_length": max(lengths) if lengths else 0,
        "most_common": most_common,
        "sample_values": values[:5]
    }


def _analyze_numeric_field(vectors: List[Dict], field_name: str) -> Dict[str, Any]:
    """ìˆ«ì í•„ë“œì˜ í†µê³„ ë¶„ì„"""
    if not vectors or field_name not in vectors[0]:
        return {"exists": False}
    
    values = []
    for v in vectors:
        val = v.get(field_name)
        if isinstance(val, (int, float)):
            values.append(val)
        elif isinstance(val, str) and val.isdigit():
            values.append(int(val))
    
    if not values:
        return {"exists": True, "error": "No numeric values found"}
    
    return {
        "exists": True,
        "count": len(values),
        "mean": float(np.mean(values)),
        "std": float(np.std(values)),
        "min": min(values),
        "max": max(values),
        "unique_count": len(set(values)),
        "sample_values": values[:10]
    }


def check_file_existence():
    """íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    _print_separator("íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸")
    
    files_to_check = [
        (TU_VEC_PATH, "Task Unit ë²¡í„° (ë©”ì¸)"),
        (TASK_VEC_PATH, "Task ë²¡í„° (ë©”ì¸)"),
        (TU_VEC_BACKUP, "Task Unit ë²¡í„° (ë°±ì—…)"),
        (TASK_VEC_BACKUP, "Task ë²¡í„° (ë°±ì—…)")
    ]
    
    for file_path, description in files_to_check:
        if os.path.exists(file_path):
            size = os.path.getsize(file_path)
            print(f"âœ… {description}")
            print(f"   ê²½ë¡œ: {file_path}")
            print(f"   í¬ê¸°: {_format_bytes(size)}")
        else:
            print(f"âŒ {description}")
            print(f"   ê²½ë¡œ: {file_path} (íŒŒì¼ ì—†ìŒ)")
        print()


def load_and_inspect_pkl(file_path: str, file_type: str):
    """PKL íŒŒì¼ ë¡œë“œ ë° êµ¬ì¡° ë¶„ì„"""
    if not os.path.exists(file_path):
        print(f"âŒ {file_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None
    
    _print_separator(f"{file_type} êµ¬ì¡° ë¶„ì„ - {file_path}")
    
    try:
        with open(file_path, 'rb') as f:
            data = pickle.load(f)
        
        print(f"âœ… íŒŒì¼ ë¡œë“œ ì„±ê³µ")
        print(f"ğŸ“Š ë°ì´í„° íƒ€ì…: {type(data)}")
        print(f"ğŸ“Š ë ˆì½”ë“œ ìˆ˜: {len(data) if isinstance(data, (list, tuple)) else 'N/A'}")
        
        if isinstance(data, list) and len(data) > 0:
            return analyze_record_structure(data, file_type)
        else:
            print("âš ï¸ ë¹ˆ ë¦¬ìŠ¤íŠ¸ì´ê±°ë‚˜ ì˜ˆìƒê³¼ ë‹¤ë¥¸ ë°ì´í„° êµ¬ì¡°ì…ë‹ˆë‹¤.")
            return data
            
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def analyze_record_structure(data: List[Dict], file_type: str):
    """ë ˆì½”ë“œ êµ¬ì¡° ìƒì„¸ ë¶„ì„"""
    if not data:
        return data
    
    first_record = data[0]
    
    _print_subsection("ê¸°ë³¸ ì •ë³´")
    print(f"ì´ ë ˆì½”ë“œ ìˆ˜: {len(data)}")
    print(f"ì²« ë²ˆì§¸ ë ˆì½”ë“œ íƒ€ì…: {type(first_record)}")
    
    if isinstance(first_record, dict):
        print(f"í•„ë“œ ìˆ˜: {len(first_record.keys())}")
        print(f"í•„ë“œ ëª©ë¡: {list(first_record.keys())}")
    
    _print_subsection("í•„ë“œë³„ ìƒì„¸ ë¶„ì„")
    
    if isinstance(first_record, dict):
        # ê° í•„ë“œ íƒ€ì…ë³„ë¡œ ë¶„ì„
        for field_name, field_value in first_record.items():
            print(f"\nğŸ” í•„ë“œ: '{field_name}'")
            print(f"   íƒ€ì…: {type(field_value)}")
            
            if isinstance(field_value, (list, np.ndarray)) and len(field_value) > 0:
                # ë²¡í„° í•„ë“œ ë¶„ì„
                if isinstance(field_value[0], (int, float)):
                    stats = _analyze_vector_field(data, field_name)
                    if stats.get("exists"):
                        print(f"   ë²¡í„° ì°¨ì›: {stats.get('dimension', 'N/A')}")
                        print(f"   í‰ê· : {stats.get('mean', 0):.6f}")
                        print(f"   í‘œì¤€í¸ì°¨: {stats.get('std', 0):.6f}")
                        print(f"   ë²”ìœ„: [{stats.get('min', 0):.6f}, {stats.get('max', 0):.6f}]")
                        print(f"   ìƒ˜í”Œ ê°’: {stats.get('sample_values', [])}")
                else:
                    print(f"   ë¦¬ìŠ¤íŠ¸ ê¸¸ì´: {len(field_value)}")
                    print(f"   ìƒ˜í”Œ ê°’: {field_value[:3] if len(field_value) >= 3 else field_value}")
            
            elif isinstance(field_value, str):
                # í…ìŠ¤íŠ¸ í•„ë“œ ë¶„ì„
                stats = _analyze_text_field(data, field_name)
                if stats.get("exists"):
                    print(f"   ë¹„ì–´ìˆì§€ ì•Šì€ ê°’: {stats.get('non_empty_count')}/{stats.get('total_count')}")
                    print(f"   í‰ê·  ê¸¸ì´: {stats.get('avg_length', 0):.1f}")
                    print(f"   ê¸¸ì´ ë²”ìœ„: [{stats.get('min_length', 0)}, {stats.get('max_length', 0)}]")
                    if stats.get('most_common'):
                        print(f"   ê°€ì¥ í”í•œ ê°’: {stats['most_common'][0] if stats['most_common'] else 'N/A'}")
            
            elif isinstance(field_value, (int, float)):
                # ìˆ«ì í•„ë“œ ë¶„ì„
                stats = _analyze_numeric_field(data, field_name)
                if stats.get("exists") and not stats.get("error"):
                    print(f"   í‰ê· : {stats.get('mean', 0):.2f}")
                    print(f"   ë²”ìœ„: [{stats.get('min', 0)}, {stats.get('max', 0)}]")
                    print(f"   ê³ ìœ  ê°’ ìˆ˜: {stats.get('unique_count', 0)}")
            
            else:
                print(f"   ê°’: {str(field_value)[:100]}{'...' if len(str(field_value)) > 100 else ''}")
    
    return data


def show_sample_records(data: List[Dict], file_type: str, num_samples: int = 2):
    """ìƒ˜í”Œ ë ˆì½”ë“œ ì¶œë ¥"""
    if not data:
        return
    
    _print_separator(f"{file_type} ìƒ˜í”Œ ë ˆì½”ë“œ")
    
    num_samples = min(num_samples, len(data))
    
    for i in range(num_samples):
        print(f"\nğŸ“‹ ìƒ˜í”Œ #{i+1}:")
        record = data[i]
        
        if isinstance(record, dict):
            for key, value in record.items():
                if isinstance(value, (list, np.ndarray)) and len(value) > 10:
                    # ê¸´ ë²¡í„°ëŠ” ì°¨ì›ê³¼ ì¼ë¶€ ê°’ë§Œ í‘œì‹œ
                    print(f"  {key}: [{len(value)}ì°¨ì› ë²¡í„°] {value[:5]}...{value[-2:]}")
                elif isinstance(value, str) and len(value) > 100:
                    # ê¸´ í…ìŠ¤íŠ¸ëŠ” ì¼ë¶€ë§Œ í‘œì‹œ
                    print(f"  {key}: {value[:100]}...")
                else:
                    print(f"  {key}: {value}")
        else:
            print(f"  {record}")
        
        if i < num_samples - 1:
            print("-" * 50)


def compare_embeddings(data: List[Dict], file_type: str):
    """ì„ë² ë”© ë²¡í„°ë“¤ ë¹„êµ ë¶„ì„"""
    if not data:
        return
    
    _print_separator(f"{file_type} ì„ë² ë”© ë¹„êµ ë¶„ì„")
    
    # ì„ë² ë”© í•„ë“œ ì°¾ê¸°
    embedding_fields = []
    first_record = data[0]
    
    for field_name, field_value in first_record.items():
        if 'embedding' in field_name or field_name == 'z_shared':
            if isinstance(field_value, (list, np.ndarray)):
                embedding_fields.append(field_name)
    
    if not embedding_fields:
        print("ì„ë² ë”© í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ë°œê²¬ëœ ì„ë² ë”© í•„ë“œ: {embedding_fields}")
    
    for field in embedding_fields:
        print(f"\nğŸ§  {field} ë¶„ì„:")
        stats = _analyze_vector_field(data, field)
        
        if stats.get("exists") and not stats.get("error"):
            print(f"   ì°¨ì›: {stats['dimension']}")
            print(f"   ìƒ˜í”Œ ìˆ˜: {stats['count']}")
            print(f"   ê°’ ë²”ìœ„: [{stats['min']:.6f}, {stats['max']:.6f}]")
            print(f"   í‰ê· : {stats['mean']:.6f}")
            print(f"   í‘œì¤€í¸ì°¨: {stats['std']:.6f}")
            
            # ë²¡í„° ê°„ ìœ ì‚¬ë„ ì²´í¬ (ì²« 5ê°œ)
            if stats['count'] >= 2:
                vectors = np.array([v[field] for v in data[:5]])
                similarities = []
                for i in range(len(vectors)):
                    for j in range(i+1, len(vectors)):
                        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                        cos_sim = np.dot(vectors[i], vectors[j]) / (
                            np.linalg.norm(vectors[i]) * np.linalg.norm(vectors[j])
                        )
                        similarities.append(cos_sim)
                
                if similarities:
                    print(f"   ìƒ˜í”Œ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í‰ê· : {np.mean(similarities):.4f}")
                    print(f"   ìƒ˜í”Œ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë²”ìœ„: [{min(similarities):.4f}, {max(similarities):.4f}]")


def export_structure_summary(tu_data: List[Dict], task_data: List[Dict]):
    """êµ¬ì¡° ìš”ì•½ì„ JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
    _print_separator("êµ¬ì¡° ìš”ì•½ ë‚´ë³´ë‚´ê¸°")
    
    summary = {
        "timestamp": __import__('time').strftime("%Y-%m-%d %H:%M:%S"),
        "task_units": {
            "count": len(tu_data) if tu_data else 0,
            "fields": list(tu_data[0].keys()) if tu_data else []
        },
        "tasks": {
            "count": len(task_data) if task_data else 0,
            "fields": list(task_data[0].keys()) if task_data else []
        }
    }
    
    # ì„ë² ë”© í•„ë“œ ì •ë³´ ì¶”ê°€
    if tu_data:
        embedding_info = {}
        for field in tu_data[0].keys():
            if 'embedding' in field or field == 'z_shared':
                if isinstance(tu_data[0][field], (list, np.ndarray)):
                    embedding_info[field] = len(tu_data[0][field])
        summary["task_units"]["embeddings"] = embedding_info
    
    if task_data:
        embedding_info = {}
        for field in task_data[0].keys():
            if 'embedding' in field or field == 'z_shared':
                if isinstance(task_data[0][field], (list, np.ndarray)):
                    embedding_info[field] = len(task_data[0][field])
        summary["tasks"]["embeddings"] = embedding_info
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    output_file = "pkl_structure_summary.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… êµ¬ì¡° ìš”ì•½ì´ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ“„ ìš”ì•½ ë‚´ìš©:")
    print(json.dumps(summary, ensure_ascii=False, indent=2))


def main():
    parser = argparse.ArgumentParser(description='PKL íŒŒì¼ êµ¬ì¡° ìƒì„¸ ë¶„ì„')
    parser.add_argument('--all', action='store_true', help='ëª¨ë“  ë¶„ì„ ìˆ˜í–‰')
    parser.add_argument('--task-units', action='store_true', help='Task Unit íŒŒì¼ë§Œ ë¶„ì„')
    parser.add_argument('--tasks', action='store_true', help='Task íŒŒì¼ë§Œ ë¶„ì„')
    parser.add_argument('--sample', type=int, default=2, help='í‘œì‹œí•  ìƒ˜í”Œ ë ˆì½”ë“œ ìˆ˜')
    parser.add_argument('--export', action='store_true', help='êµ¬ì¡° ìš”ì•½ JSON ë‚´ë³´ë‚´ê¸°')
    parser.add_argument('--files-only', action='store_true', help='íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸')
    
    args = parser.parse_args()
    
    if args.files_only:
        check_file_existence()
        return
    
    tu_data = None
    task_data = None
    
    if args.all or args.task_units or (not args.tasks and not args.all):
        tu_data = load_and_inspect_pkl(TU_VEC_PATH, "Task Unit")
        if tu_data:
            show_sample_records(tu_data, "Task Unit", args.sample)
            compare_embeddings(tu_data, "Task Unit")
    
    if args.all or args.tasks or (not args.task_units and not args.all):
        task_data = load_and_inspect_pkl(TASK_VEC_PATH, "Task")
        if task_data:
            show_sample_records(task_data, "Task", args.sample)
            compare_embeddings(task_data, "Task")
    
    if args.export and (tu_data or task_data):
        export_structure_summary(tu_data, task_data)
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸ì€ í•­ìƒ ë§ˆì§€ë§‰ì—
    if args.all:
        check_file_existence()


if __name__ == '__main__':
    main()