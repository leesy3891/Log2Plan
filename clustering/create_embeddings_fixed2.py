# -*- coding: utf-8 -*-
"""
create_embeddings_fixed_v2.py â€” ê¸°ì¡´ ì¶œë ¥(PKL, ë©”íƒ€ë°ì´í„° ë“±)ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ
ìš”ì²­í•œ í¬ë§·ì˜ JSON(Task / Task Unit)ë„ ì¶”ê°€ë¡œ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸.

v2 ë³€ê²½ì  (ì¤‘ìš” ë²„ê·¸í”½ìŠ¤)
- Task Unitì˜ "Description"ì´ ë’¤ìª½ ì œì–´ê³¼ì •/Titleê¹Œì§€ ëŒë ¤ì˜¤ëŠ” ë¬¸ì œ ìˆ˜ì •
  -> unit ë¸”ë¡ì—ì„œ Descriptionë§Œ ê¹”ë”íˆ ì˜ë¼ë‚´ëŠ” í—¬í¼(_extract_unit_description) ì¶”ê°€
  -> ì •ì œëœ descriptionì„ desc_textì™€ JSON ì €ì¥ì— ì‚¬ìš©

âœ” ê¸°ëŒ€ ë™ì‘
- ê¸°ì¡´ create_embeddings.pyì™€ ë™ì¼í•˜ê²Œ ì„ë² ë”© ìƒì„± ë° ë‹¤ìŒ íŒŒì¼ ìœ ì§€/ìƒì„±:
  - task_unit_vectors.pkl, task_vectors.pkl, embeddings_metadata.json ë“±
- ì¶”ê°€ ì¶œë ¥(JSON)
  - ./steps_json/task_units_fixed.jsonl   (Task Unit í¬ë§·)
  - ./steps_json/tasks_fixed.jsonl        (Task í¬ë§·)

ì‹¤í–‰ ì˜ˆì‹œ:
  python create_embeddings_fixed_v2.py --run
  python create_embeddings_fixed_v2.py --check

í•„ìˆ˜ ë””ë ‰í† ë¦¬/íŒŒì¼:
  - labeled_logs/*.txt
  - openaikey.env (OPENAI_API_KEY í¬í•¨)
"""

import os
import re
import glob
import json
import time
import argparse
import pickle
from pathlib import Path
from typing import List, Dict, Tuple, Any

from dotenv import load_dotenv
load_dotenv('./openaikey.env')

try:
    from openai import OpenAI
    _client = OpenAI()
except Exception:
    _client = None  # ì˜¤í”„ë¼ì¸ í™•ì¸ ëª¨ë“œì—ì„œë„ ë™ì‘í•˜ë„ë¡ í—ˆìš©

# =========================
# ê³µí†µ ìœ í‹¸
# =========================

def _read_logs() -> List[Dict[str, Any]]:
    logs = []
    for p in glob.glob('labeled_logs/*.txt'):
        try:
            with open(p, 'r', encoding='utf-8') as f:
                logs.append({
                    'file_path': p,
                    'file_name': os.path.basename(p),
                    'content': f.read()
                })
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {p} - {e}")
    if not logs:
        print("âš ï¸ labeled_logs ë””ë ‰í† ë¦¬ì— .txt ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"ğŸ“‚ ë¡œê·¸ íŒŒì¼ {len(logs)}ê°œ ì½ìŒ")
    return logs

_env_pat = re.compile(r'ENV\[(.*?)\]')
_act_pat = re.compile(r'ACT\[(.*?)\]')


def _parse_tags(name: str) -> Tuple[List[str], str, str]:
    env_match = _env_pat.search(name)
    act_match = _act_pat.search(name)

    env_raw = env_match.group(1).strip() if env_match else ''
    env_tags = [e.strip() for e in env_raw.split(',')] if env_raw else []
    act_tag = act_match.group(1).strip() if act_match else ''

    # Title = ë§ˆì§€ë§‰ "]" ë’¤ì˜ ë³¸ë¬¸ (ê´„í˜¸ì˜ (0~29) ê°™ì€ ë²”ìœ„ í‘œí˜„ì€ ì œê±°)
    title_part = re.sub(r'.*ACT\[[^\]]*\]\s*', '', name).strip()
    title = re.sub(r'\s*\(.*?\)\s*$', '', title_part).strip()
    return env_tags, act_tag, title


def _extract_unit_block(content: str, unit_num: str) -> str:
    m = re.search(rf'Task Unit #({unit_num}):([\s\S]*?)(?=Task Unit #\d+:|$)', content)
    return (m.group(0).strip() if m else '')


def _extract_all_numbered_steps(unit_content: str) -> Dict[int, str]:
    """Unit ì „ì²´ì—ì„œ ë²ˆí˜¸ê°€ ìˆëŠ” ëª¨ë“  stepë“¤ì„ ì¶”ì¶œ.
    ì €ì¥ ì‹œì—ëŠ” ë²ˆí˜¸ ì ‘ë‘ë¶€("n.")ë¥¼ ì œê±°í•˜ê³  0#ë¶€í„° ì‹œì‘í•˜ëŠ” í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©.
    """
    steps = {}
    for line in unit_content.splitlines():
        line = line.strip()
        m = re.match(r'^(\d+)\.\s*(0#.+)$', line)
        if m:
            idx = int(m.group(1))
            zero_hash = m.group(2)  # ì˜ˆ: 0#press, win
            steps[idx] = zero_hash
    return steps


def _extract_step_range_from_task(task_content: str) -> Tuple[int, int]:
    m = re.search(r'\((\d+)~(\d+)\)', task_content)
    if m:
        return int(m.group(1)), int(m.group(2))
    return None, None


def _extract_unit_description(unit_block: str) -> str:
    """Task Unit ë¸”ë¡ì—ì„œ Descriptionë§Œ ê¹”ë”íˆ ì¶”ì¶œ.
    - "Description:" ì´í›„ í…ìŠ¤íŠ¸ë¥¼ ì²˜ìŒ ë¹ˆ ì¤„(\n\n)ì´ë‚˜ êµ¬íš ì‹ í˜¸(\nTask#, \nENV[, \nACT[, \nTitle:) ì „ê¹Œì§€ë¡œ ì œí•œ
    - ë’¤ì„ì—¬ ë“¤ì–´ì˜¤ëŠ” Title/ì œì–´ ê³¼ì •ì´ ì„ì´ì§€ ì•Šë„ë¡ ë°©ì§€
    """
    m = re.search(r'Description:\s*([\s\S]*)', unit_block)
    if not m:
        return ''
    tail = m.group(1)
    cut_candidates = []
    for pat in [r'\n\s*\n', r'\nTask#', r'\nENV\[', r'\nACT\[', r'\nTitle:']:
        mm = re.search(pat, tail)
        if mm:
            cut_candidates.append(mm.start())
    cut_at = min(cut_candidates) if cut_candidates else len(tail)
    desc = tail[:cut_at].strip()
    # ê°™ì€ ì¤„ ë§ë¯¸ì— Titleì´ ë¶™ì€ ê²½ìš° ëŒ€ë¹„
    desc = re.sub(r'Title:.*$', '', desc).strip()
    return desc


def _extract_units_and_tasks(logs: List[Dict[str, Any]]):
    task_units = []
    tasks = []

    # description ìº¡ì²˜ëŠ” í•˜ë˜, ì‹¤ì œ ì €ì¥ì—ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³  unit_blockì—ì„œ ì¬ê³„ì‚°ëœ description ì‚¬ìš©
    unit_pat = re.compile(r'Task Unit #(\d+): \*\*(.*?)\*\*\s+Description: ([\s\S]*?)(?=Task Unit #\d+:|$)')
    task_pat_inside_unit = re.compile(r'(Task#\d+:[\s\S]*?)(?=\nTask#\d+:|$)')

    for log in logs:
        content = log['content']
        file_name = log['file_name']
        units = unit_pat.findall(content)
        for _, (unit_num, name, _desc_ignored) in enumerate(units, start=1):
            unit_uid = f"TU_{len(task_units):04d}"
            unit_block = _extract_unit_block(content, unit_num)
            all_steps_map = _extract_all_numbered_steps(unit_block)

            # ê°œë³„ Task ë¸”ë¡ë“¤
            task_blocks = task_pat_inside_unit.findall(unit_block)
            env_tags, act_tag, title = _parse_tags(name)

            # Description ì •ì œ (ì¤‘ìš”!)
            description = _extract_unit_description(unit_block)

            # Task Unit JSON ë ˆì½”ë“œ ì¤€ë¹„ (steps: íƒ­ ì—°ê²°)
            unit_steps_list = [all_steps_map[k] for k in sorted(all_steps_map.keys())]
            tu_json = {
                "unique_id": unit_uid,
                "file_name": file_name,
                "unit_number": str(unit_num),
                "name": name.strip(),
                "description": description.strip(),
                "task_count": len(task_blocks),
                "env_tag": env_tags,
                "act_tag": act_tag,
                "title": title,
                "env_text": ", ".join(env_tags) if env_tags else "unknown",
                "act_text": act_tag if act_tag else "unknown",
                "desc_text": f"{title} - {description.strip()}" if title and description.strip() else (title or description.strip() or "unknown"),
                "steps": "\t".join(unit_steps_list) if unit_steps_list else "",
            }
            task_units.append(tu_json)

            # Task JSON ë ˆì½”ë“œë“¤
            for t_idx, task_block in enumerate(task_blocks):
                t_uid = f"T_{len(tasks):05d}"
                env_m = _env_pat.search(task_block)
                act_m = _act_pat.search(task_block)
                desc_m = re.search(r'Description:\s*(.*?)(?=\n|$)', task_block, re.MULTILINE)

                env_text = env_m.group(1) if env_m else "unknown"
                act_text = act_m.group(1) if act_m else "unknown"
                desc_text = (desc_m.group(1).strip() if desc_m else "unknown")

                s, e = _extract_step_range_from_task(task_block)
                step_range = f"({s}~{e})" if s is not None else ""
                chosen_steps = []
                if s is not None and e is not None:
                    for k in range(s, e + 1):
                        if k in all_steps_map:
                            chosen_steps.append(all_steps_map[k])  # ë²ˆí˜¸ ì œê±°ëœ "0#..."ë§Œ íƒ­ ì—°ê²°

                task_json = {
                    "unique_id": t_uid,
                    "parent_unit_id": unit_uid,
                    "parent_unit_name": title,
                    "env_text": env_text,
                    "act_text": act_text,
                    "desc_text": desc_text,
                    "file_name": file_name,
                    "step_range": step_range,
                    "step_count": len(chosen_steps),
                    "steps": "\t".join(chosen_steps) if chosen_steps else "",
                }
                tasks.append(task_json)

    return task_units, tasks

# =========================
# ì„ë² ë”© (ê¸°ì¡´ ì¶œë ¥ ìœ ì§€)
# =========================

def _emb_batch(texts: List[str], model: str = "text-embedding-3-large", batch_size: int = 20):
    if _client is None:
        # í…ŒìŠ¤íŠ¸/ì²´í¬ ëª¨ë“œ: ë”ë¯¸ ë²¡í„°
        import numpy as np
        dim = 3072 if model == "text-embedding-3-large" else 1536
        return [np.zeros(dim, dtype=float).tolist() for _ in texts]

    all_vecs = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        try:
            resp = _client.embeddings.create(model=model, input=batch)
            all_vecs.extend([d.embedding for d in resp.data])
        except Exception as e:
            print(f"âš ï¸ ë°°ì¹˜ ì„ë² ë”© ì‹¤íŒ¨ @ {i}: {e}")
            # ê°œë³„ ë³´ì •
            for t in batch:
                try:
                    r = _client.embeddings.create(model=model, input=[t])
                    all_vecs.append(r.data[0].embedding)
                except Exception:
                    import numpy as np
                    dim = 3072 if model == "text-embedding-3-large" else 1536
                    all_vecs.append(np.random.normal(0, 0.1, dim).tolist())
    return all_vecs


def _attach_embeddings_task_units(tus: List[Dict[str, Any]], model: str = "text-embedding-3-large"):
    env = [', '.join(tu['env_tag']) if tu['env_tag'] else 'unknown' for tu in tus]
    act = [tu['act_tag'] if tu['act_tag'] else 'unknown' for tu in tus]
    des = [tu['desc_text'] for tu in tus]

    env_e = _emb_batch(env, model)
    act_e = _emb_batch(act, model)
    des_e = _emb_batch(des, model)

    out = []
    for i, tu in enumerate(tus):
        item = dict(tu)
        item['env_embedding'] = env_e[i]
        item['act_embedding'] = act_e[i]
        item['des_embedding'] = des_e[i]
        out.append(item)
    return out


def _attach_embeddings_tasks(ts: List[Dict[str, Any]], model: str = "text-embedding-3-large"):
    env = [t['env_text'] for t in ts]
    act = [t['act_text'] for t in ts]
    des = [t['desc_text'] for t in ts]

    env_e = _emb_batch(env, model)
    act_e = _emb_batch(act, model)
    des_e = _emb_batch(des, model)

    out = []
    for i, t in enumerate(ts):
        item = dict(t)
        item['env_embedding'] = env_e[i]
        item['act_embedding'] = act_e[i]
        item['des_embedding'] = des_e[i]
        out.append(item)
    return out


# =========================
# ì €ì¥ ë£¨í‹´
# =========================

def _ensure_dir(p: str):
    Path(p).mkdir(parents=True, exist_ok=True)


def _save_jsonl(path: str, records: List[Dict[str, Any]]):
    with open(path, 'w', encoding='utf-8') as f:
        for rec in records:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
    print(f"   âœ… {os.path.basename(path)} ì €ì¥ (lines={len(records)})")


def _save_pkl(task_units_with_emb: List[Dict[str, Any]], tasks_with_emb: List[Dict[str, Any]]):
    # content/stepsëŠ” JSONì— ì´ë¯¸ ìˆìœ¼ë¯€ë¡œ PKLì€ ì„ë² ë”© ì¤‘ì‹¬ ìœ ì§€ (ì› ì½”ë“œ í˜¸í™˜)
    def strip_steps(items):
        out = []
        for it in items:
            d = {k: v for k, v in it.items() if k != 'steps'}
            out.append(d)
        return out

    tu_pkl = strip_steps(task_units_with_emb)
    t_pkl = strip_steps(tasks_with_emb)

    with open('task_unit_vectors.pkl', 'wb') as f:
        pickle.dump(tu_pkl, f)
    with open('task_vectors.pkl', 'wb') as f:
        pickle.dump(t_pkl, f)

    # ë ˆê±°ì‹œ íŒŒì¼ë„ ë™ì¼í•˜ê²Œ ìœ ì§€
    with open('task_unit_vectors_legacy.pkl', 'wb') as f:
        pickle.dump(tu_pkl, f)
    with open('task_vectors_legacy.pkl', 'wb') as f:
        pickle.dump(t_pkl, f)

    print("   âœ… PKL ì €ì¥ ì™„ë£Œ (ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ í˜¸í™˜)")


def _save_metadata(logs_cnt: int, tu_cnt: int, t_cnt: int, dim: int, extra: Dict[str, Any] = None):
    meta = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "total_task_units": tu_cnt,
        "total_tasks": t_cnt,
        "embedding_model": "text-embedding-3-large",
        "embedding_dimension": dim,
        "content_in_json": True,
        "pkl_content_excluded": True,
    }
    if extra:
        meta.update(extra)
    with open('embeddings_metadata.json', 'w', encoding='utf-8') as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    print("   âœ… embeddings_metadata.json ì €ì¥ ì™„ë£Œ")


# =========================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# =========================

def run_pipeline():
    logs = _read_logs()
    if not logs:
        return False

    print("ğŸ“Š Task Unit / Task ì¶”ì¶œâ€¦")
    task_units, tasks = _extract_units_and_tasks(logs)
    print(f"   Task Units: {len(task_units)}  |  Tasks: {len(tasks)}")

    print("ğŸ§  ì„ë² ë”© ìƒì„±â€¦ (ENV/ACT/DES)")
    tus_with_emb = _attach_embeddings_task_units(task_units)
    ts_with_emb = _attach_embeddings_tasks(tasks)

    print("ğŸ’¾ ê¸°ì¡´ ì¶œë ¥ ìœ ì§€(PKL) ì €ì¥â€¦")
    dim = len(tus_with_emb[0]['env_embedding']) if tus_with_emb else 0
    _save_pkl(tus_with_emb, ts_with_emb)

    print("ğŸ“ JSON ìƒì„± (ìš”ì²­ í¬ë§·)â€¦")
    outdir = './steps_json'
    _ensure_dir(outdir)

    # ìš”ì²­ í¬ë§·ì— ë§ì¶˜ JSONL ì €ì¥
    _save_jsonl(os.path.join(outdir, 'task_units_fixed.jsonl'), task_units)
    _save_jsonl(os.path.join(outdir, 'tasks_fixed.jsonl'), tasks)

    _save_metadata(
        logs_cnt=len(logs),
        tu_cnt=len(task_units),
        t_cnt=len(tasks),
        dim=dim,
        extra={
            "json_outputs": {
                "task_units_fixed": os.path.join(outdir, 'task_units_fixed.jsonl'),
                "tasks_fixed": os.path.join(outdir, 'tasks_fixed.jsonl'),
            }
        }
    )

    print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    return True


def check_outputs():
    files = [
        'task_unit_vectors.pkl', 'task_vectors.pkl', 'embeddings_metadata.json',
        './steps_json/task_units_fixed.jsonl', './steps_json/tasks_fixed.jsonl'
    ]
    print("ğŸ“¦ ìƒì„±ë¬¼ ì ê²€:")
    for p in files:
        if os.path.exists(p):
            print(f"  âœ… {p} â€” {os.path.getsize(p):,} bytes")
        else:
            print(f"  âŒ {p} â€” ì—†ìŒ")


if __name__ == '__main__':
    ap = argparse.ArgumentParser()
    ap.add_argument('--run', action='store_true', help='íŒŒì´í”„ë¼ì¸ ì‹¤í–‰(ì„ë² ë”©+PKL+JSON)')
    ap.add_argument('--check', action='store_true', help='ì¶œë ¥ë¬¼ ì¡´ì¬ ì—¬ë¶€ ì ê²€')
    args = ap.parse_args()

    if args.check:
        check_outputs()
    else:
        run_pipeline()