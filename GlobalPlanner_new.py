import openai
import os
import glob
import re
import pickle
import time
import numpy as np
from collections import defaultdict
from sklearn.cluster import DBSCAN
from sklearn.metrics import pairwise_distances
from dotenv import load_dotenv

# ìºì‹œ ì‹œìŠ¤í…œ import
from long_term_plan_cache import LongTermPlanCache, bayesian_avg

load_dotenv('./openaikey.env')
openai_api_key = os.getenv("OPENAI_API_KEY")

if openai.api_key:
    print("API í‚¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

from openai import OpenAI
client = openai.OpenAI()

# ==================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ====================

def cosine_similarity(a, b):
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    a = np.array(a)
    b = np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8)

def embed(text):
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
    return client.embeddings.create(
        model="text-embedding-3-large", 
        input=[text]
    ).data[0].embedding

def create_embeddings(texts):
    """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë°°ì¹˜ í…ìŠ¤íŠ¸ ì„ë² ë”© (ë‹¨ì¼ ì„ë² ë”©)"""
    batch_size = 20
    all_embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        response = client.embeddings.create(
            model="text-embedding-3-large",
            input=batch
        )
        all_embeddings.extend([item.embedding for item in response.data])
    
    return all_embeddings

def create_embeddings_separated(texts_env, texts_act, texts_des):
    """
    ENV, ACT, DESë¥¼ ê°œë³„ì ìœ¼ë¡œ ì„ë² ë”© ìƒì„±
    """
    batch_size = 20
    
    def embed_batch(texts):
        all_embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            response = client.embeddings.create(
                model="text-embedding-3-large",
                input=batch
            )
            all_embeddings.extend([item.embedding for item in response.data])
        return all_embeddings
    
    print("ENV íƒœê·¸ ì„ë² ë”© ìƒì„± ì¤‘...")
    env_embeddings = embed_batch(texts_env)
    
    print("ACT íƒœê·¸ ì„ë² ë”© ìƒì„± ì¤‘...")
    act_embeddings = embed_batch(texts_act)
    
    print("Description ì„ë² ë”© ìƒì„± ì¤‘...")
    des_embeddings = embed_batch(texts_des)
    
    return env_embeddings, act_embeddings, des_embeddings

def weighted_similarity(item1, item2, env_weight=0.3, act_weight=0.2, des_weight=0.5):
    """
    ENV:ACT:DES = 3:2:5 ê°€ì¤‘í•©ìœ¼ë¡œ ìœ ì‚¬ë„ ê³„ì‚°
    """
    env_sim = cosine_similarity(item1['env_embedding'], item2['env_embedding'])
    act_sim = cosine_similarity(item1['act_embedding'], item2['act_embedding'])
    des_sim = cosine_similarity(item1['des_embedding'], item2['des_embedding'])
    
    return env_weight * env_sim + act_weight * act_sim + des_weight * des_sim

def weighted_query_similarity(query_emb, item, env_weight=0.3, act_weight=0.2, des_weight=0.5):
    """
    ì¿¼ë¦¬ì™€ ì•„ì´í…œ ê°„ ê°€ì¤‘ ìœ ì‚¬ë„ ê³„ì‚°
    """
    if isinstance(query_emb, dict):
        # ì¿¼ë¦¬ë„ ë¶„ë¦¬ëœ ì„ë² ë”©ì¸ ê²½ìš°
        env_sim = cosine_similarity(query_emb['env'], item['env_embedding'])
        act_sim = cosine_similarity(query_emb['act'], item['act_embedding'])
        des_sim = cosine_similarity(query_emb['des'], item['des_embedding'])
    else:
        # ì¿¼ë¦¬ê°€ ë‹¨ì¼ ì„ë² ë”©ì¸ ê²½ìš° (ê¸°ì¡´ í˜¸í™˜ì„±)
        env_sim = cosine_similarity(query_emb, item['env_embedding'])
        act_sim = cosine_similarity(query_emb, item['act_embedding'])
        des_sim = cosine_similarity(query_emb, item['des_embedding'])
    
    return env_weight * env_sim + act_weight * act_sim + des_weight * des_sim

# ==================== ë¡œê·¸ íŒŒì‹± ====================

def read_labeled_logs():
    """labeled_logs ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ë¡œê·¸ íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤."""
    logs = []
    for file_path in glob.glob('labeled_logs/*.txt'):
        with open(file_path, 'r', encoding='utf-8') as f:
            logs.append({"file_path": file_path, "content": f.read()})
    return logs

def extract_task_unit_headers(gpt_output: str) -> str:
    """GPT ì¶œë ¥ì—ì„œ Task Unit í—¤ë”ë§Œ ì¶”ì¶œ"""
    pattern = r'Task\s*Unit\s*:?\s*(\*\*.*?\*\*)'
    headers = re.findall(pattern, gpt_output)
    if not headers:
        print("âš  No Task Unit headers matched.")
    return "\n".join([f"Task Unit : {h}" for h in headers])

def extract_tasks(unit_text: str):
    """Task Unitì—ì„œ ê°œë³„ Task# ë¸”ë¡ë“¤ì„ ì¶”ì¶œ"""
    pattern = r'(Task#\d+:[\s\S]*?)(?=\nTask#\d+:|$)'
    return [m.group(1).strip() for m in re.finditer(pattern, unit_text)]

def extract_task_units(logs):
    """ë¡œê·¸ì—ì„œ Task Unit ì¶”ì¶œ ë° íŒŒì‹±"""
    task_units = []
    
    for log in logs:
        content = log['content']
        file_name = os.path.basename(log['file_path'])
        
        # Task Unit íŒ¨í„´ ë§¤ì¹­
        pattern = r'Task Unit #(\d+): \*\*(.*?)\*\*\s+Description: (.*?)(?=Task#\d+:|$)'
        matches = re.findall(pattern, content, re.DOTALL)
        
        for idx, (unit_num, name, description) in enumerate(matches, start=1):
            unit_id = f"{file_name}:Unit{unit_num}"
            unit_start = content.find(f"Task Unit #{unit_num}")
            next_header = f"Task Unit #{int(unit_num)+1}"
            unit_end = content.find(next_header) if next_header in content else len(content)
            unit_content = content[unit_start:unit_end].strip()
            
            env_tags, act_tag, title = parse_tags(name)
            
            task_units.append({
                "id": unit_id,
                "file_path": log['file_path'],
                "name": name.strip(),
                "description": description.strip(),
                "content": unit_content,
                "env_tag": env_tags,
                "act_tag": act_tag,
                "title": title
            })
    
    return task_units

def parse_tags(name):
    """ENV[], ACT[], Title íŒŒì‹±"""
    env_match = re.search(r'ENV\[(.*?)\]', name)
    act_match = re.search(r'ACT\[(.*?)\]', name)
    
    env_raw = env_match.group(1).strip() if env_match else ''
    env_tags = [e.strip() for e in env_raw.split(',')] if env_raw else []
    
    act_tag = act_match.group(1).strip() if act_match else ''
    
    # Title = ACT] ë’¤ì˜ ë¶€ë¶„
    title_part = re.sub(r'.*ACT\[[^\]]*\]\s*', '', name).strip()
    title = re.sub(r'\s*\(.*?\)\s*$', '', title_part).strip()
    
    return env_tags, act_tag, title

# ==================== DBSCAN ë²¡í„° DB ====================

def create_vector_db(eps_factor=1.5, min_samples=3, auto_adjust=True):
    """
    ê°œì„ ëœ DBSCAN í´ëŸ¬ìŠ¤í„°ë§ì„ ì‚¬ìš©í•œ ë²¡í„° DB ìƒì„±
    ENV, ACT, DESë¥¼ ê°œë³„ ì„ë² ë”©ìœ¼ë¡œ ì²˜ë¦¬
    """
    db_file = "task_unit_vectors_dbscan_separated.pkl"
    
    if os.path.exists(db_file):
        print("ê¸°ì¡´ ë²¡í„° DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        with open(db_file, "rb") as f:
            return pickle.load(f)
    
    print("ìƒˆë¡œìš´ ë²¡í„° DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    
    # 1. ë¡œê·¸ ì½ê¸° ë° Task Unit ì¶”ì¶œ
    logs = read_labeled_logs()
    print(f"{len(logs)}ê°œì˜ ë¡œê·¸ íŒŒì¼ì„ ì½ì—ˆìŠµë‹ˆë‹¤.")
    
    task_units = extract_task_units(logs)
    print(f"{len(task_units)}ê°œì˜ ì‘ì—… ë‹¨ìœ„ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
    
    # 2. ENV, ACT, DES í…ìŠ¤íŠ¸ ë¶„ë¦¬
    texts_env = []
    texts_act = []
    texts_des = []
    
    for unit in task_units:
        # ENV: í™˜ê²½ íƒœê·¸ë“¤ì„ ì‰¼í‘œë¡œ ì—°ê²°
        env_text = ', '.join(unit['env_tag']) if unit['env_tag'] else 'unknown'
        texts_env.append(env_text)
        
        # ACT: ì•¡ì…˜ íƒœê·¸
        act_text = unit['act_tag'] if unit['act_tag'] else 'unknown'
        texts_act.append(act_text)
        
        # DES: ì œëª© + ì„¤ëª… ê²°í•©
        des_text = f"{unit['title']} - {unit['description']}" if unit['title'] and unit['description'] else unit['title'] or unit['description'] or 'unknown'
        texts_des.append(des_text)
    
    # 3. ê°œë³„ ì„ë² ë”© ìƒì„±
    print("ENV, ACT, DES ê°œë³„ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    env_embeddings, act_embeddings, des_embeddings = create_embeddings_separated(
        texts_env, texts_act, texts_des
    )
    
    # 4. í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ ê²°í•© ì„ë² ë”© ìƒì„± (ê°€ì¤‘ í‰ê· )
    print("í´ëŸ¬ìŠ¤í„°ë§ìš© ê²°í•© ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    combined_embeddings = []
    for i in range(len(task_units)):
        # ENV:ACT:DES = 3:2:5 ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ê²°í•©
        env_emb = np.array(env_embeddings[i])
        act_emb = np.array(act_embeddings[i])
        des_emb = np.array(des_embeddings[i])
        
        combined = 0.3 * env_emb + 0.2 * act_emb + 0.5 * des_emb
        combined_embeddings.append(combined.tolist())
    
    # 5. DBSCAN í´ëŸ¬ìŠ¤í„°ë§ (ê²°í•© ì„ë² ë”© ì‚¬ìš©)
    print("DBSCAN í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
    
    # ê°œì„ ëœ eps ê³„ì‚° ë°©ì‹
    dists = pairwise_distances(combined_embeddings, metric="cosine")
    tri = dists[np.triu_indices_from(dists, k=1)]
    
    # ê¸°ë³¸ í†µê³„
    median, std = np.median(tri), np.std(tri)
    percentile_25 = np.percentile(tri, 25)
    percentile_75 = np.percentile(tri, 75)
    
    # ë” ê´€ëŒ€í•œ eps ê³„ì‚° (ì—¬ëŸ¬ ë°©ë²• ì¤‘ ìµœëŒ€ê°’ ì„ íƒ)
    eps_methods = [
        median + eps_factor * std,           # ê¸°ì¡´ ë°©ì‹ (ë” í° ê³„ìˆ˜)
        percentile_25 + 0.3 * (percentile_75 - percentile_25),  # IQR ê¸°ë°˜
        np.mean(tri) + 0.8 * std             # í‰ê·  ê¸°ë°˜
    ]
    eps = float(max(eps_methods))  # ê°€ì¥ ê´€ëŒ€í•œ ê°’ ì„ íƒ
    
    print(f"ê±°ë¦¬ í†µê³„ - ì¤‘ì•™ê°’: {median:.4f}, í‘œì¤€í¸ì°¨: {std:.4f}")
    print(f"DBSCAN íŒŒë¼ë¯¸í„°: eps={eps:.4f}, min_samples={min_samples}")
    
    # ìë™ ì¡°ì • ëª¨ë“œ
    best_clustering = None
    best_params = None
    
    if auto_adjust:
        print("ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤...")
        
        # ì—¬ëŸ¬ íŒŒë¼ë¯¸í„° ì¡°í•© ì‹œë„
        param_combinations = [
            (eps, min_samples),
            (eps * 1.2, min_samples),
            (eps * 1.5, min_samples),
            (eps, max(2, min_samples - 1)),
            (eps * 1.3, max(2, min_samples - 1))
        ]
        
        for test_eps, test_min_samples in param_combinations:
            dbscan = DBSCAN(eps=test_eps, min_samples=test_min_samples, metric='cosine')
            test_labels = dbscan.fit_predict(combined_embeddings)
            
            n_clusters = len(set(test_labels)) - (1 if -1 in test_labels else 0)
            n_noise = list(test_labels).count(-1)
            noise_ratio = n_noise / len(test_labels)
            
            # ì¢‹ì€ í´ëŸ¬ìŠ¤í„°ë§ ì¡°ê±´: 
            # 1) 3ê°œ ì´ìƒì˜ í´ëŸ¬ìŠ¤í„°
            # 2) ë…¸ì´ì¦ˆ ë¹„ìœ¨ 70% ë¯¸ë§Œ
            # 3) í´ëŸ¬ìŠ¤í„°ë‹¹ í‰ê·  ì•„ì´í…œ ìˆ˜ê°€ ì ì ˆí•¨
            if n_clusters >= 3 and noise_ratio < 0.7:
                avg_cluster_size = (len(test_labels) - n_noise) / max(n_clusters, 1)
                if avg_cluster_size >= 2:  # í´ëŸ¬ìŠ¤í„°ë‹¹ ìµœì†Œ 2ê°œ ì•„ì´í…œ
                    best_clustering = test_labels
                    best_params = (test_eps, test_min_samples)
                    print(f"âœ… ì ì ˆí•œ íŒŒë¼ë¯¸í„° ë°œê²¬: eps={test_eps:.4f}, min_samples={test_min_samples}")
                    print(f"   í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}, ë…¸ì´ì¦ˆ ë¹„ìœ¨: {noise_ratio:.1%}")
                    break
            
            print(f"   ì‹œë„: eps={test_eps:.4f}, min_samples={test_min_samples} â†’ í´ëŸ¬ìŠ¤í„°:{n_clusters}, ë…¸ì´ì¦ˆ:{noise_ratio:.1%}")
    
    # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§
    if best_clustering is not None:
        cluster_labels = best_clustering
        eps, min_samples = best_params
    else:
        print("âš ï¸ ìë™ ì¡°ì • ì‹¤íŒ¨. ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©")
        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
        cluster_labels = dbscan.fit_predict(combined_embeddings)
    
    # ìµœì¢… í´ëŸ¬ìŠ¤í„° ì •ë³´
    unique_labels = set(cluster_labels)
    n_clusters = len(unique_labels) - (1 if -1 in cluster_labels else 0)
    n_noise = list(cluster_labels).count(-1)
    
    print(f"ğŸ“Š ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:")
    print(f"   í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}")
    print(f"   ë…¸ì´ì¦ˆ í¬ì¸íŠ¸: {n_noise} ({n_noise/len(cluster_labels):.1%})")
    
    # í´ëŸ¬ìŠ¤í„°ë³„ ìƒì„¸ ì •ë³´
    if n_clusters > 0:
        cluster_sizes = {}
        for label in cluster_labels:
            if label != -1:  # ë…¸ì´ì¦ˆ ì œì™¸
                cluster_sizes[label] = cluster_sizes.get(label, 0) + 1
        
        print(f"   í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„í¬: {dict(sorted(cluster_sizes.items()))}")
    
    # 6. ë²¡í„° DB êµ¬ì„± (ê°œë³„ ì„ë² ë”© + ê²°í•© ì„ë² ë”© ëª¨ë‘ ì €ì¥)
    vector_db = []
    for i, unit in enumerate(task_units):
        vector_db.append({
            "id": unit["id"],
            "file_path": unit["file_path"],
            "content": unit["content"],
            "env_tag": unit["env_tag"],
            "act_tag": unit["act_tag"],
            "title": unit["title"],
            "description": unit["description"],
            "env_embedding": env_embeddings[i],      # ENV ê°œë³„ ì„ë² ë”©
            "act_embedding": act_embeddings[i],      # ACT ê°œë³„ ì„ë² ë”©
            "des_embedding": des_embeddings[i],      # DES ê°œë³„ ì„ë² ë”©
            "embedding": combined_embeddings[i],     # ê²°í•© ì„ë² ë”© (ê¸°ì¡´ í˜¸í™˜ì„±)
            "cluster": int(cluster_labels[i])        # DBSCAN í´ëŸ¬ìŠ¤í„° ID
        })
    
    # 7. ì €ì¥
    with open(db_file, "wb") as f:
        pickle.dump(vector_db, f)
    
    print(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ {db_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return vector_db

# ==================== ë‹¤ì–‘ì„± ê¸°ë°˜ ê²€ìƒ‰ ====================

def search_hier_mmr(query_emb, vector_db, top_clusters=5, k_per=3, lambda_div=0.4, fallback_to_similarity=True):
    """
    ê°œì„ ëœ í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ MMR ê²€ìƒ‰ (ê°€ì¤‘ ìœ ì‚¬ë„ ì‚¬ìš©)
    """
    # í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ì„ë² ë”© ê³„ì‚° (ë…¸ì´ì¦ˆ ì œì™¸)
    reps = defaultdict(list)
    noise_items = []  # ë…¸ì´ì¦ˆ ì•„ì´í…œ ë³„ë„ ë³´ê´€
    
    for v in vector_db:
        if v['cluster'] != -1:  # ì •ìƒ í´ëŸ¬ìŠ¤í„°
            reps[v['cluster']].append(v['embedding'])
        else:  # ë…¸ì´ì¦ˆ ì•„ì´í…œ
            noise_items.append(v)
    
    # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬ ì„ë² ë”©
    rep_embs = {cid: np.mean(rep, axis=0) for cid, rep in reps.items() if rep}
    
    print(f"ğŸ” í™œìš© ê°€ëŠ¥í•œ í´ëŸ¬ìŠ¤í„°: {len(rep_embs)}ê°œ, ë…¸ì´ì¦ˆ ì•„ì´í…œ: {len(noise_items)}ê°œ")
    
    # í´ëŸ¬ìŠ¤í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´
    if len(rep_embs) < 2 and fallback_to_similarity:
        print("âš ï¸ í´ëŸ¬ìŠ¤í„° ìˆ˜ ë¶€ì¡± â†’ ê°€ì¤‘ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´")
        return search_by_weighted_similarity(query_emb, vector_db, top_k=top_clusters * k_per)
    
    # í´ëŸ¬ìŠ¤í„° ìˆœìœ„ ë§¤ê¸°ê¸° (ê°€ì¤‘ ìœ ì‚¬ë„ ì‚¬ìš©)
    cid_rank = []
    for cid in rep_embs:
        # í´ëŸ¬ìŠ¤í„° ëŒ€í‘œì™€ ì¿¼ë¦¬ ê°„ ê°€ì¤‘ ìœ ì‚¬ë„
        if isinstance(query_emb, dict):
            # ì¿¼ë¦¬ê°€ ë¶„ë¦¬ëœ ì„ë² ë”©ì¸ ê²½ìš°
            sim = cosine_similarity(query_emb.get('combined', query_emb.get('embedding')), rep_embs[cid])
        else:
            # ì¿¼ë¦¬ê°€ ë‹¨ì¼ ì„ë² ë”©ì¸ ê²½ìš°
            sim = cosine_similarity(query_emb, rep_embs[cid])
        cid_rank.append((sim, cid))
    
    cid_rank.sort(key=lambda x: x[0], reverse=True)
    cid_rank = [cid for _, cid in cid_rank[:top_clusters]]
    
    picked = []
    
    # ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ MMR ì„ íƒ (ê°€ì¤‘ ìœ ì‚¬ë„ ì‚¬ìš©)
    for cid in cid_rank:
        cand = [v for v in vector_db if v['cluster'] == cid]
        
        # ê°€ì¤‘ ìœ ì‚¬ë„ë¡œ ì ìˆ˜ ê³„ì‚°
        scores = []
        for v in cand:
            if isinstance(query_emb, dict):
                sim = weighted_query_similarity(query_emb, v)
            else:
                # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•´ ê²°í•© ì„ë² ë”© ì‚¬ìš©
                sim = cosine_similarity(query_emb, v['embedding'])
            scores.append((v, sim))
        
        # MMR ì„ íƒ
        selected = []
        while scores and len(selected) < k_per:
            mmr = []
            for v, s in scores:
                # ê°€ì¤‘ ìœ ì‚¬ë„ë¡œ ë‹¤ì–‘ì„± ê³„ì‚°
                max_sim = 0
                for x in selected:
                    div_sim = weighted_similarity(v, x)
                    max_sim = max(max_sim, div_sim)
                
                mmr_score = s - lambda_div * max_sim
                mmr.append((mmr_score, v))
            
            v_best = max(mmr, key=lambda t: t[0])[1]
            selected.append(v_best)
            scores = [(v, s) for v, s in scores if v is not v_best]
        
        picked.extend(selected)
    
    # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ë…¸ì´ì¦ˆ ì•„ì´í…œì—ì„œ ë³´ì¶©
    if len(picked) < top_clusters * k_per // 2 and noise_items:
        print(f"ğŸ”„ ê²°ê³¼ ë¶€ì¡± â†’ ë…¸ì´ì¦ˆ ì•„ì´í…œ {len(noise_items)}ê°œì—ì„œ ë³´ì¶©")
        
        noise_scores = []
        for v in noise_items:
            if isinstance(query_emb, dict):
                sim = weighted_query_similarity(query_emb, v)
            else:
                sim = cosine_similarity(query_emb, v['embedding'])
            noise_scores.append((v, sim))
        
        noise_scores.sort(key=lambda x: x[1], reverse=True)
        
        # ê¸°ì¡´ ì„ íƒê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ë…¸ì´ì¦ˆ ì•„ì´í…œ ì¶”ê°€
        needed = (top_clusters * k_per) - len(picked)
        for v, score in noise_scores[:needed * 2]:
            if v not in picked:
                picked.append(v)
                if len(picked) >= top_clusters * k_per:
                    break
    
    print(f"âœ… ìµœì¢… ì„ íƒëœ ì•„ì´í…œ: {len(picked)}ê°œ")
    return picked

def search_by_weighted_similarity(query_emb, vector_db, top_k=15):
    """
    ê°€ì¤‘ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ (í´ë°± ì˜µì…˜)
    """
    print("ğŸ“Š ê°€ì¤‘ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
    
    # ëª¨ë“  ì•„ì´í…œì— ëŒ€í•´ ê°€ì¤‘ ìœ ì‚¬ë„ ê³„ì‚°
    scored = []
    for v in vector_db:
        if isinstance(query_emb, dict):
            sim = weighted_query_similarity(query_emb, v)
        else:
            sim = cosine_similarity(query_emb, v['embedding'])
        scored.append((sim, v))
    
    # ìœ ì‚¬ë„ìˆœ ì •ë ¬
    scored.sort(key=lambda x: x[0], reverse=True)
    
    # ë‹¤ì–‘ì„±ì„ ìœ„í•œ ê°„ë‹¨í•œ í•„í„°ë§ (ê°€ì¤‘ ìœ ì‚¬ë„ ì‚¬ìš©)
    selected = []
    threshold = 0.95  # ë„ˆë¬´ ìœ ì‚¬í•œ ê²ƒë“¤ ì œê±°
    
    for sim, v in scored:
        # ì´ë¯¸ ì„ íƒëœ ê²ƒê³¼ ë„ˆë¬´ ìœ ì‚¬í•˜ì§€ ì•Šì€ì§€ í™•ì¸
        too_similar = False
        for selected_v in selected:
            if weighted_similarity(v, selected_v) > threshold:
                too_similar = True
                break
        
        if not too_similar:
            selected.append(v)
            if len(selected) >= top_k:
                break
    
    print(f"âœ… ê°€ì¤‘ ìœ ì‚¬ë„ ê¸°ë°˜ ì„ íƒ: {len(selected)}ê°œ")
    return selected
    
    for v in vector_db:
        if v['cluster'] != -1:  # ì •ìƒ í´ëŸ¬ìŠ¤í„°
            reps[v['cluster']].append(v['embedding'])
        else:  # ë…¸ì´ì¦ˆ ì•„ì´í…œ
            noise_items.append(v)
    
    # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬ ì„ë² ë”©
    rep_embs = {cid: np.mean(rep, axis=0) for cid, rep in reps.items() if rep}
    
    print(f"ğŸ” í™œìš© ê°€ëŠ¥í•œ í´ëŸ¬ìŠ¤í„°: {len(rep_embs)}ê°œ, ë…¸ì´ì¦ˆ ì•„ì´í…œ: {len(noise_items)}ê°œ")
    
    # í´ëŸ¬ìŠ¤í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´
    if len(rep_embs) < 2 and fallback_to_similarity:
        print("âš ï¸ í´ëŸ¬ìŠ¤í„° ìˆ˜ ë¶€ì¡± â†’ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´")
        return search_by_similarity_only(query_emb, vector_db, top_k=top_clusters * k_per)
    
    # í´ëŸ¬ìŠ¤í„° ìˆœìœ„ ë§¤ê¸°ê¸°
    cid_rank = sorted(
        rep_embs, 
        key=lambda c: cosine_similarity(query_emb, rep_embs[c]), 
        reverse=True
    )[:top_clusters]
    
    picked = []
    
    # ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ MMR ì„ íƒ
    for cid in cid_rank:
        cand = [v for v in vector_db if v['cluster'] == cid]
        scores = [(v, cosine_similarity(query_emb, v['embedding'])) for v in cand]
        
        # MMR ì„ íƒ
        selected = []
        while scores and len(selected) < k_per:
            mmr = []
            for v, s in scores:
                max_sim = max([
                    cosine_similarity(v['embedding'], x['embedding']) 
                    for x in selected
                ] or [0])
                
                mmr_score = s - lambda_div * max_sim
                mmr.append((mmr_score, v))
            
            v_best = max(mmr, key=lambda t: t[0])[1]
            selected.append(v_best)
            scores = [(v, s) for v, s in scores if v is not v_best]
        
        picked.extend(selected)
    
    # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ë…¸ì´ì¦ˆ ì•„ì´í…œì—ì„œ ë³´ì¶©
    if len(picked) < top_clusters * k_per // 2 and noise_items:
        print(f"ğŸ”„ ê²°ê³¼ ë¶€ì¡± â†’ ë…¸ì´ì¦ˆ ì•„ì´í…œ {len(noise_items)}ê°œì—ì„œ ë³´ì¶©")
        
        noise_scores = [
            (v, cosine_similarity(query_emb, v['embedding'])) 
            for v in noise_items
        ]
        noise_scores.sort(key=lambda x: x[1], reverse=True)
        
        # ê¸°ì¡´ ì„ íƒê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ë…¸ì´ì¦ˆ ì•„ì´í…œ ì¶”ê°€
        needed = (top_clusters * k_per) - len(picked)
        for v, score in noise_scores[:needed * 2]:  # ì—¬ìœ ìˆê²Œ í›„ë³´ í™•ë³´
            if v not in picked:
                picked.append(v)
                if len(picked) >= top_clusters * k_per:
                    break
    
    print(f"âœ… ìµœì¢… ì„ íƒëœ ì•„ì´í…œ: {len(picked)}ê°œ")
    return picked

def search_by_similarity_only(query_emb, vector_db, top_k=15):
    """
    í´ëŸ¬ìŠ¤í„°ë§ ì—†ì´ ìˆœìˆ˜ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ (í´ë°± ì˜µì…˜)
    """
    print("ğŸ“Š ìˆœìˆ˜ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
    
    # ëª¨ë“  ì•„ì´í…œì— ëŒ€í•´ ìœ ì‚¬ë„ ê³„ì‚°
    scored = []
    for v in vector_db:
        sim = cosine_similarity(query_emb, v['embedding'])
        scored.append((sim, v))
    
    # ìœ ì‚¬ë„ìˆœ ì •ë ¬
    scored.sort(key=lambda x: x[0], reverse=True)
    
    # ë‹¤ì–‘ì„±ì„ ìœ„í•œ ê°„ë‹¨í•œ í•„í„°ë§
    selected = []
    threshold = 0.95  # ë„ˆë¬´ ìœ ì‚¬í•œ ê²ƒë“¤ ì œê±°
    
    for sim, v in scored:
        # ì´ë¯¸ ì„ íƒëœ ê²ƒê³¼ ë„ˆë¬´ ìœ ì‚¬í•˜ì§€ ì•Šì€ì§€ í™•ì¸
        too_similar = False
        for selected_v in selected:
            if cosine_similarity(v['embedding'], selected_v['embedding']) > threshold:
                too_similar = True
                break
        
        if not too_similar:
            selected.append(v)
            if len(selected) >= top_k:
                break
    
    print(f"âœ… ìœ ì‚¬ë„ ê¸°ë°˜ ì„ íƒ: {len(selected)}ê°œ")
    return selected

def search_diverse_similar_tasks(query: str, vector_db: list, top_k: int = 9):
    """
    ë‹¨ê³„ì  ë‹¤ì´ë²„ì‹œí‹° ì„ íƒ (ê°€ì¤‘ ìœ ì‚¬ë„ ì‚¬ìš©)
    """
    query = query.strip()
    if not query:
        raise ValueError("âŒ Provided query is empty. Cannot create embedding.")
    
    # ì¿¼ë¦¬ë¥¼ ENV, ACT, DESë¡œ ë¶„ë¦¬í•˜ì—¬ ì„ë² ë”© ìƒì„±
    print("ğŸ” ì¿¼ë¦¬ ë¶„ì„ ë° ë¶„ë¦¬ëœ ì„ë² ë”© ìƒì„±...")
    
    # ê°„ë‹¨í•œ ì¿¼ë¦¬ íŒŒì‹± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•  ìˆ˜ ìˆìŒ)
    env_text = "unknown"  # ê¸°ë³¸ê°’
    act_text = "unknown"  # ê¸°ë³¸ê°’
    des_text = query      # ì „ì²´ ì¿¼ë¦¬ë¥¼ ì„¤ëª…ìœ¼ë¡œ ì‚¬ìš©
    
    # Task Unit í—¤ë”ì—ì„œ ENV, ACT ì¶”ì¶œ ì‹œë„
    env_match = re.search(r'ENV\[(.*?)\]', query)
    act_match = re.search(r'ACT\[(.*?)\]', query)
    
    if env_match:
        env_text = env_match.group(1)
    if act_match:
        act_text = act_match.group(1)
        # ACT ì´í›„ ë¶€ë¶„ì„ ì„¤ëª…ìœ¼ë¡œ ì‚¬ìš©
        des_text = re.sub(r'.*ACT\[[^\]]*\]\s*', '', query).strip()
    
    # ê°œë³„ ì„ë² ë”© ìƒì„±
    query_env_emb = embed(env_text)
    query_act_emb = embed(act_text)
    query_des_emb = embed(des_text)
    
    query_emb_dict = {
        'env': query_env_emb,
        'act': query_act_emb,
        'des': query_des_emb,
        'combined': 0.3 * np.array(query_env_emb) + 0.2 * np.array(query_act_emb) + 0.5 * np.array(query_des_emb)
    }
    
    # ê°€ì¤‘ ìœ ì‚¬ë„ ê³„ì‚° ë° ì •ë ¬
    scored = []
    for idx, item in enumerate(vector_db):
        sim = weighted_query_similarity(query_emb_dict, item)
        scored.append((idx, sim))
    scored.sort(key=lambda x: x[1], reverse=True)
    
    selected, excluded = [], set()
    
    def exclude_most_similar(to_idx):
        """ê°€ì¥ ìœ ì‚¬í•œ í•­ëª© ì œì™¸ (ê°€ì¤‘ ìœ ì‚¬ë„ ì‚¬ìš©)"""
        base_item = vector_db[to_idx]
        best_j, best_sim = None, -1
        for j, _ in scored:
            if j in excluded or j == to_idx:
                continue
            sim = weighted_similarity(base_item, vector_db[j])
            if sim > best_sim:
                best_sim, best_j = sim, j
        if best_j is not None:
            excluded.add(best_j)
    
    stage_targets = [3, 3, 3]  # 3-3-3
    for stage_size in stage_targets:
        for idx, _ in scored:
            if idx in excluded:
                continue
            selected.append(idx)
            excluded.add(idx)
            exclude_most_similar(idx)
            if len(selected) % stage_size == 0:
                break
    
    selected = selected[:top_k]
    return [vector_db[i] for i in selected]

def top_similar_tasks_per_current(content_text: str, candidate_units: list, top_n: int = 2):
    """
    í˜„ì¬ Task#ë³„ë¡œ ê°€ì¥ ìœ ì‚¬í•œ í›„ë³´ Task# ì„ íƒ (ê°€ì¤‘ ìœ ì‚¬ë„ ì‚¬ìš©)
    """
    cur_tasks = extract_tasks(content_text)
    if not cur_tasks:
        return []
    
    # í›„ë³´ Task# ì „ë¶€ ìˆ˜ì§‘
    cand_tasks = []
    for unit in candidate_units:
        for t in extract_tasks(unit['content']):
            cand_tasks.append(t)
    
    # í˜„ì¬ Taskë“¤ì„ ENV, ACT, DESë¡œ ë¶„ë¦¬í•˜ì—¬ ì„ë² ë”©
    print("ğŸ” í˜„ì¬ Taskë“¤ì˜ ë¶„ë¦¬ëœ ì„ë² ë”© ìƒì„±...")
    cur_env_texts, cur_act_texts, cur_des_texts = [], [], []
    
    for task in cur_tasks:
        # Taskì—ì„œ ENV, ACT ì¶”ì¶œ
        env_match = re.search(r'ENV\[(.*?)\]', task)
        act_match = re.search(r'ACT\[(.*?)\]', task)
        desc_match = re.search(r'Description:\s*(.*?)(?=\n|$)', task)
        
        env_text = env_match.group(1) if env_match else "unknown"
        act_text = act_match.group(1) if act_match else "unknown"
        desc_text = desc_match.group(1) if desc_match else task[:100]  # ì²« 100ìë¥¼ ì„¤ëª…ìœ¼ë¡œ
        
        cur_env_texts.append(env_text)
        cur_act_texts.append(act_text)
        cur_des_texts.append(desc_text)
    
    # í›„ë³´ Taskë“¤ë„ ë§ˆì°¬ê°€ì§€ë¡œ ë¶„ë¦¬
    cand_env_texts, cand_act_texts, cand_des_texts = [], [], []
    
    for task in cand_tasks:
        env_match = re.search(r'ENV\[(.*?)\]', task)
        act_match = re.search(r'ACT\[(.*?)\]', task)
        desc_match = re.search(r'Description:\s*(.*?)(?=\n|$)', task)
        
        env_text = env_match.group(1) if env_match else "unknown"
        act_text = act_match.group(1) if act_match else "unknown"
        desc_text = desc_match.group(1) if desc_match else task[:100]
        
        cand_env_texts.append(env_text)
        cand_act_texts.append(act_text)
        cand_des_texts.append(desc_text)
    
    # ëª¨ë“  í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
    all_env_texts = cur_env_texts + cand_env_texts
    all_act_texts = cur_act_texts + cand_act_texts
    all_des_texts = cur_des_texts + cand_des_texts
    
    env_embs, act_embs, des_embs = create_embeddings_separated(all_env_texts, all_act_texts, all_des_texts)
    
    # ë¶„ë¦¬
    cur_env_embs = env_embs[:len(cur_tasks)]
    cur_act_embs = act_embs[:len(cur_tasks)]
    cur_des_embs = des_embs[:len(cur_tasks)]
    
    cand_env_embs = env_embs[len(cur_tasks):]
    cand_act_embs = act_embs[len(cur_tasks):]
    cand_des_embs = des_embs[len(cur_tasks):]
    
    # Taskë³„ top-n ì„ ì • (ê°€ì¤‘ ìœ ì‚¬ë„ ì‚¬ìš©)
    selected = []
    for i in range(len(cur_tasks)):
        sims = []
        for j in range(len(cand_tasks)):
            # ê°€ì¤‘ ìœ ì‚¬ë„ ê³„ì‚°
            env_sim = cosine_similarity(cur_env_embs[i], cand_env_embs[j])
            act_sim = cosine_similarity(cur_act_embs[i], cand_act_embs[j])
            des_sim = cosine_similarity(cur_des_embs[i], cand_des_embs[j])
            
            weighted_sim = 0.3 * env_sim + 0.2 * act_sim + 0.5 * des_sim
            sims.append((weighted_sim, cand_tasks[j]))
        
        sims.sort(key=lambda x: x[0], reverse=True)
        top_examples = [t for _, t in sims[:top_n]]
        selected.extend(top_examples)
    
    return selected

# ==================== Coherence ì ìˆ˜ (ê¸°ì¡´ ë¡œì§) ====================

ACT_TRANSITION = defaultdict(int)
TRANS_PROB = {}

def build_act_transition(vector_db):
    """ACT ì „ì´ í™•ë¥  ê³„ì‚°"""
    for v in vector_db:
        acts = []
        for t in extract_tasks(v['content']):
            match = re.search(r'ACT\[(.*?)\]', t)
            if match:
                acts.append(match.group(1))
        
        for a, b in zip(acts, acts[1:]):
            ACT_TRANSITION[(a, b)] += 1
    
    total = sum(ACT_TRANSITION.values())
    for k, v in ACT_TRANSITION.items():
        TRANS_PROB[k] = v / total

def coherence(path):
    """ê²½ë¡œì˜ coherence ì ìˆ˜ ê³„ì‚°"""
    acts = []
    for t in path:
        match = re.search(r'ACT\[(.*?)\]', t)
        if match:
            acts.append(match.group(1))
    
    if len(acts) < 2:
        return 0
    
    return np.mean([
        TRANS_PROB.get((acts[i], acts[i+1]), 0) 
        for i in range(len(acts)-1)
    ])

def fill_missing(path, query_emb, vector_db):
    """ëˆ„ë½ëœ ACT ì‘ì—… ë³´ê°•"""
    seen = set()
    for p in path:
        match = re.search(r'ACT\[(.*?)\]', p)
        if match:
            seen.add(match.group(1))
    
    for v in vector_db:
        if v['act_tag'] in seen:
            continue
        if cosine_similarity(query_emb, v['embedding']) > 0.8:
            path.append(v['content'])
            seen.add(v['act_tag'])
    
    return path

# ==================== GPT ëª…ë ¹ì–´ ì²˜ë¦¬ ====================

def process_command(command):
    """GPT APIë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…ë ¹ì–´ë¥¼ êµ¬ì¡°í™”ëœ Task Unitìœ¼ë¡œ ë³€í™˜"""
    prompt = f"""Your job is to break down a command given by the user, which can be executed in user's Desktop, Windows. 
    Identify tasks by breaking down the command, and label them. Task Unit containes a sequence of Tasks to execute the command.

    Create a structured labeling in the following format:
    
    Task Unit : **ENV[environment/platform-subdomains] ACT[action_category/specific_action] Descriptive Title**  

    Task#1: ENV[environment/platform-subdomains] ACT[action]  
    Description: [Brief description of this specific subtask, max 20 words]

    Task#2: ENV[environment/platform-subdomains] ACT[action] 
    Description: [Brief description of this specific subtask, max 20 words]

    [List of numbered tasks goes here, keeping original numbering]

    Always use "Task Unit : **ENV[] ACT[] Title** format because I need to parse your response.
    Follow these rules for the labeling:

    1. ENV tag format:
       - local/[program] - For local desktop applications (e.g., local/FileExplorer)
       - web/[sitename] - For web browsing (e.g., web/Chrome-University Portal, web/Chrome-Shopping Platform)
       - app/[appname] - For specific applications (e.g., app/Excel, app/VSCode)
       
    2. ACT tag format:
    - [action1, action2,..]
    Examples:
    | ACT Tag |
    |------------|
    | ACT[search, filter_sort] |
    | ACT[add_to_cart, product_purchase] |
    | ACT[open_document] |

    3. For Task Units, make a sequence of environments which are fit for fulfilling user's command, if multiple envs needs to be involved:
       - ENV[web/Chrome-a Web to get a summary] when a user requests a summary of a paper
       - ENV[local/FileExplorer,app/Excel] for a unit involving File Explorer and Excel

    4. Use flexible, semantically meaningful ACT tags based on behavior, not only from a fixed list.

    User Command:
    {command}"""

    try:
        print("GPT API í˜¸ì¶œ ì¤‘...")
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are an expert at analyzing and breaking down commands into task."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=2500,
            temperature=0.4
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"âŒ OpenAI API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return None

# ==================== ê²°ê³¼ í† í°í™” ====================

def tokenizer(output):
    """LLM ì¶œë ¥ì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ íŒŒì‹±"""
    parsed_list = []
    for line in output.strip().split("\n"):
        match = re.match(r"(\d+)\.\s*(\d+)#([^,]+),\s*(.+)", line)
        if match:
            assist_bit = match.group(2).strip()
            action = match.group(3).strip()
            obj = match.group(4).strip()
            parsed_list.append([assist_bit, action, obj])
        else:
            print(f"Warning: Unrecognized task format - {line}")
    
    return parsed_list

# ==================== ë¡œê·¸ ê¸°ë¡ ====================

def log_result(query, similar_units, output, llm_output, log_dir="result_logs"):
    """ìƒì„± ê³¼ì •ê³¼ ê²°ê³¼ë¥¼ ë¡œê·¸ íŒŒì¼ë¡œ ì €ì¥"""
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"result_{timestamp}.txt")
    
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write(f"Query: {query}\n\n")
        
        # ìœ ì‚¬í•œ Task Unit ì •ë³´
        f.write(f"Similar Task Units ({len(similar_units)}):\n")
        for i, unit in enumerate(similar_units):
            cluster_info = f"Cluster[{unit.get('cluster', 'N/A')}]"
            f.write(f"{i+1}. {unit['id']} - {cluster_info} ENV[{unit['env_tag']}] ACT[{unit['act_tag']}] Title[{unit.get('title', '')}]\n")
        f.write("\n")
        
        # LLM ì¶œë ¥
        f.write("LLM Output:\n")
        f.write(llm_output)
        f.write("\n\n")
        
        # ìµœì¢… íŒŒì‹±ëœ ê²°ê³¼
        f.write("Final Parsed Output:\n")
        for i, step in enumerate(output):
            f.write(f"{i+1}. {step[0]}#{step[1]}, {step[2]}\n")
    
    print(f"ê²°ê³¼ê°€ {log_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return log_file

# ==================== ì‚¬ìš©ì í”¼ë“œë°± ====================

def get_user_feedback(plan_id: str) -> int:
    """1-5ì  í”¼ë“œë°± ìˆ˜ì§‘"""
    print("\nğŸ“ í”Œëœ í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”:")
    print("1ì : ë§¤ìš° ë‚˜ì¨ (ì „í˜€ ì‘ë™í•˜ì§€ ì•ŠìŒ)")
    print("2ì : ë‚˜ì¨ (ë§ì€ ì˜¤ë¥˜ ìˆìŒ)")
    print("3ì : ë³´í†µ (ì¼ë¶€ ì‘ë™í•¨)")
    print("4ì : ì¢‹ìŒ (ëŒ€ë¶€ë¶„ ì˜ ì‘ë™í•¨)")
    print("5ì : ë§¤ìš° ì¢‹ìŒ (ì™„ë²½í•˜ê²Œ ì‘ë™í•¨)")
    
    while True:
        try:
            score = int(input("ì ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (1-5): "))
            if 1 <= score <= 5:
                cache = LongTermPlanCache()
                cache.add_feedback(plan_id, score)
                print(f"âœ… {score}ì  í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                cache.close()
                return score
            else:
                print("1-5 ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        except ValueError:
            print("ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# ==================== ë©”ì¸ GlobalPlanner ====================

def globalPlanner(command, content, use_cache=True, sim_thresh=0.75, quality_cut=4.0, top_k_cache=5):
    """
    í†µí•©ëœ GlobalPlanner - DBSCAN í´ëŸ¬ìŠ¤í„°ë§ + ìºì‹œ ì‹œìŠ¤í…œ
    """
    MAGENTA = "\033[95m"
    CYAN = "\033[96m"
    YELLOW = "\033[93m"
    RESET = "\033[0m"
    
    # ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    cache = LongTermPlanCache()
    plan_id = None
    negative_examples = []
    
    try:
        # ëª…ë ¹ì–´ ì„ë² ë”© ìƒì„±
        command_emb = np.array(embed(command))
        
        # 1. ìºì‹œ ì¡°íšŒ
        if use_cache:
            cached_plans = cache.get_candidates(
                command, command_emb,
                top_k=top_k_cache,
                sim_thresh=sim_thresh,
                include_archived=False
            )
            
            if cached_plans:
                print(f"{CYAN}ğŸ” ìºì‹œì—ì„œ {len(cached_plans)}ê°œì˜ ìœ ì‚¬í•œ í”Œëœì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.{RESET}")
                
                high_q, low_q = [], []
                for p in cached_plans:
                    q = bayesian_avg(p.score_sum, p.score_cnt)
                    (high_q if q >= quality_cut else low_q).append((q, p))
                
                if high_q:  # ê³ í’ˆì§ˆ í”Œëœ ë°”ë¡œ ì‚¬ìš©
                    high_q.sort(key=lambda x: x[0], reverse=True)
                    best_quality, best_plan = high_q[0]
                    print(f"{CYAN}ğŸ“‹ ê³ í’ˆì§ˆ ìºì‹œ í”Œëœ ì‚¬ìš© (í’ˆì§ˆ={best_quality:.2f}, ì‚¬ìš©={best_plan.score_cnt}){RESET}")
                    
                    # ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
                    if isinstance(best_plan.plan_json, dict) and "tasks" in best_plan.plan_json:
                        output = best_plan.plan_json["tasks"]
                    else:
                        output = tokenizer(str(best_plan.plan_json))
                    
                    plan_id = best_plan.plan_id
                    
                    # ìºì‹œëœ í”Œëœ ë‚´ìš© í‘œì‹œ
                    print(f"\nğŸ“‹ ìºì‹œëœ í”Œëœ ë‚´ìš©:")
                    for i, task in enumerate(output, 1):
                        print(f"  {i}. {task[0]}#{task[1]}, {task[2]}")
                    
                    # í”¼ë“œë°± ìš”ì²­
                    get_user_feedback(plan_id)
                    return output, plan_id
                
                else:
                    negative_examples = [p for _, p in low_q]
                    print(f"{YELLOW}âš ï¸ ê³ í’ˆì§ˆ ì—†ìŒ â†’ ì €í’ˆì§ˆ {len(negative_examples)}ê°œë¥¼ ë¶€ì • ì˜ˆì‹œë¡œ ì‚¬ìš©{RESET}")
        
        # 2. ìƒˆë¡œìš´ í”Œëœ ìƒì„±
        print(f"{CYAN}ğŸ”„ ìƒˆë¡œìš´ í”Œëœì„ ìƒì„±í•©ë‹ˆë‹¤...{RESET}")
        
        # 2-1. ë¶€ì • ì˜ˆì‹œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        negative_prompt = ""
        if negative_examples:
            neg_tasks = []
            for neg_plan in negative_examples[:2]:
                pj = neg_plan.plan_json
                if isinstance(pj, dict):
                    if "tasks" in pj:
                        neg_tasks.extend(pj["tasks"][:5])
                    elif "llm_output" in pj:
                        neg_tasks.extend(tokenizer(pj["llm_output"])[:5])
            
            if neg_tasks:
                neg_text = "\n".join(
                    f"{i+1}. {t[0]}#{t[1]}, {t[2]}" for i, t in enumerate(neg_tasks)
                )
                negative_prompt = (
                    "ë‹¤ìŒì€ ê³¼ê±° ë‚®ì€ í‰ê°€(â‰¤3ì )ë¥¼ ë°›ì€ ì ‘ê·¼ ì˜ˆì‹œì…ë‹ˆë‹¤. "
                    "ì´ íŒ¨í„´ì„ ë°˜ë³µí•˜ì§€ ë§ê³  ë” ë‚˜ì€ ê³„íšì„ ì œì‹œí•˜ì„¸ìš”.\n"
                    f"{neg_text}\n"
                    "ìœ„ì™€ ê°™ì€ ìˆœì„œ/í–‰ë™/ì„¤ëª…ì„ í”¼í•˜ì„¸ìš”."
                )
        
        # 2-2. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ë° ê²€ìƒ‰ (ê°œì„ ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        vector_db = create_vector_db(eps_factor=1.5, min_samples=3, auto_adjust=True)
        build_act_transition(vector_db)
        
        print("ìœ ì‚¬í•œ ì‘ì—…ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
        query_text = extract_task_unit_headers(content)
        
        # ì¿¼ë¦¬ë¥¼ ENV, ACT, DESë¡œ ë¶„ë¦¬í•˜ì—¬ ì„ë² ë”© ìƒì„±
        print("ğŸ” ì¿¼ë¦¬ ë¶„ì„ ë° ë¶„ë¦¬ëœ ì„ë² ë”© ìƒì„±...")
        
        env_text = "unknown"
        act_text = "unknown" 
        des_text = query_text
        
        # Task Unit í—¤ë”ì—ì„œ ENV, ACT ì¶”ì¶œ
        env_match = re.search(r'ENV\[(.*?)\]', query_text)
        act_match = re.search(r'ACT\[(.*?)\]', query_text)
        
        if env_match:
            env_text = env_match.group(1)
        if act_match:
            act_text = act_match.group(1)
            des_text = re.sub(r'.*ACT\[[^\]]*\]\s*', '', query_text).strip()
        
        # ë¶„ë¦¬ëœ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_env_emb = embed(env_text)
        query_act_emb = embed(act_text)
        query_des_emb = embed(des_text)
        
        query_emb_dict = {
            'env': query_env_emb,
            'act': query_act_emb, 
            'des': query_des_emb,
            'combined': (0.3 * np.array(query_env_emb) + 
                        0.2 * np.array(query_act_emb) + 
                        0.5 * np.array(query_des_emb)).tolist()
        }
        
        print(f"ğŸ“Š ë¶„ë¦¬ëœ ì„ë² ë”© ìƒì„± ì™„ë£Œ - ENV: '{env_text}', ACT: '{act_text}', DES: '{des_text[:50]}...'")
        
        # MMR ê¸°ë°˜ ê²€ìƒ‰ (ê°€ì¤‘ ìœ ì‚¬ë„ ì‚¬ìš©)
        similar_units = search_hier_mmr(query_emb_dict, vector_db, top_clusters=5, k_per=3)
        print(f"{len(similar_units)}ê°œì˜ ìœ ì‚¬í•œ ì‘ì—…ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        # Taskë³„ ì˜ˆì‹œ ì„ íƒ
        examples_list = top_similar_tasks_per_current(content, similar_units, top_n=2)
        
        # Coherence ê¸°ë°˜ ì •ë ¬
        examples_list = sorted(examples_list, key=lambda seq: coherence([seq]), reverse=True)
        
        # ëˆ„ë½ëœ ACT ë³´ê°• (ê²°í•© ì„ë² ë”© ì‚¬ìš©)
        filled_examples = fill_missing(examples_list, query_emb_dict['combined'], vector_db)
        
        examples_text = "\n\n".join(filled_examples)
        print(f"{CYAN}Examples to show to the prompt: {examples_text}{RESET}")
        
        # 2-3. LLM ì‹œìŠ¤í…œ ë©”ì‹œì§€ êµ¬ì„±
        sys_msgs = [
            {"role": "system", "content": "you are a Desktop automation agent GUI control automation task. your OS is Windows 10. make a list of subtasks which is a combination of the given Events to fulfill the given task."},
            {"role": "system", "content": "Each subtask is in 'assist bit# Event, objects' form. User assist bit is a bit to tell if a Event should be done by the user or target for the task was not given. It should normally be 0, but should be 1 when user action is needed. ex: 'input user ID and PW', 'drag the area you want to copy', 'doubleclick the version that fits'. object is the target of the action."},
            {"role": "system", "content": "Given Events(#18): click/rightclick/drag/scroll/press/text input/open/close/switch focus/go-to/save/copy/paste/delete/rename/login/repeat/wait.  ex: 0#switch focus, back to Excel sheet, 0#click, Image button"},
            {"role": "system", "content": "As an exception, when the Event is repeat, the subtask should be in a form of 'assist bit# repeat, task#a-b, on object_A' like '9. 0#repeat, 3-8, on fileB, 10. 0#repeat, 3-8, on fileC...'. Event: switch focus is when focused window needs change(not needed when focus is naturally changed by the previous Event, like opening Chrome). Event: go-to means navigation to another website inside current window.(not needed when navigation happens by the previous Event, like clicking a hyperlink)"},
            {"role": "system", "content": "if subtask is 'press (blank)', (blank) should be KEYBOARD_KEYS in PyautoGUI."},
            {"role": "system", "content": "If possible, choose using shortcut keys on Windows over click/rightclick/doubleclick. ex:use 'f2' instead of 'click 'Rename', 'ctrl+shift+n' to create new folder, 'ctrl+g' to group selected objects.' Use Chrome as your browser on Web tasks, and open files/apps directely using without traversing through file explorer. (EX: 1. 0#press, win\n2. 0#text input, Filename.txt\n 3. press, enter)"},
            {"role": "system", "content": "IMPORTANT: Avoid unnecessary use of 'drag'. Drag events often occur meaninglessly in user logs, so do not insert 'drag' unless it clearly reflects a user action like selecting or moving specific items."},
            {"role": "system", "content": "Here is one example, Example: execute kakaotalk -> (1. 0#open, kakaotalk\n2. 1#login, on kakaotalk)"},
            {"role": "system", "content": "You are an automation planner AI. You should take inspiration from past task approach. Do NOT just copy and be creative."},
            {"role": "system", "content": "You are given multiple past Task Units. Each Task Unit contains tasks. You must **select the most relevant individual tasks**, and combine them to create a new plan"},
            {"role": "system", "content": "Choose the best matching task(s) from different Task Units."},
            {"role": "system", "content": "Follow these steps: 1. Analyze the user request and infer needed ENV and ACT. 2. From the given examples, extract useful task(s) (e.g., Task#1 from Task Unit A, Task#2 from Task Unit B, etc.). 3. Reorder and adjust tasks to match the goal logically and efficiently."},
            {"role": "system", "content": "IMPORTANT: Format your output EXACTLY as follows, with no additional text, headings, or explanations:\n1. 0#press, win\n2. 0#text input, chrome\n3. 0#press, enter\n...\n\nDo not include any other text or explanation - ONLY the numbered list with the exact format shown above."}
        ]
        
        if negative_prompt:
            sys_msgs.insert(1, {"role": "system", "content": negative_prompt})
        
        user_msgs = [
            {"role": "user", "content": f"Here are some related past Task for inspiration (these are just examples, not exact solutions):\n{examples_text}"},
            {"role": "user", "content": f"Task: {command}"}
        ]
        
        messages = sys_msgs + user_msgs
        
        # 2-4. LLM í˜¸ì¶œ
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[m for m in messages if m],
            max_tokens=4000,
            temperature=0.4
        )
        
        print(f"Given command: {content}")
        llm_output = response.choices[0].message.content
        print(f"{MAGENTA}Task Splitted: {llm_output}{RESET}")
        
        output = tokenizer(llm_output)
        
        # 2-5. ìƒˆë¡œìš´ í”Œëœì„ ìºì‹œì— ì €ì¥
        retrieved_log_ids = [task.get('id', 'unknown') for task in similar_units]
        plan_json = {"tasks": output, "llm_output": llm_output}
        
        plan_id = cache.insert_plan(
            command_text=command,
            command_emb=command_emb,
            retrieved_logs=retrieved_log_ids,
            plan_json=plan_json
        )
        
        print(f"{CYAN}ğŸ’¾ ìƒˆë¡œìš´ í”Œëœì´ ìºì‹œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ID: {plan_id[:8]}...{RESET}")
        
        # 3. ì‚¬ìš©ì í”¼ë“œë°±
        get_user_feedback(plan_id)
        
        # 4. ê²°ê³¼ ë¡œê·¸ ì €ì¥
        log_result(content, similar_units, output, llm_output)
        
        return output, plan_id
    
    finally:
        cache.close()

# ==================== ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ====================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=== í†µí•©ëœ GlobalPlanner (DBSCAN + ìºì‹œ) ===")
    
    # ì‚¬ìš©ì ëª…ë ¹ì–´ ì…ë ¥
    command = input("Command: ")
    print(f"Given command: {command}")
    
    # GPTë¡œ ëª…ë ¹ì–´ ì²˜ë¦¬
    content = process_command(command)
    if not content:
        print("âŒ ëª…ë ¹ì–´ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    print(f"Processed Command: {content}")
    
    # GlobalPlanner ì‹¤í–‰
    try:
        task_list, plan_id = globalPlanner(command, content)
        
        print(f"\nğŸ¯ ìµœì¢… ìƒì„±ëœ ì‘ì—… ìˆ˜: {len(task_list)}")
        print("\nğŸ“‹ ìµœì¢… ì‘ì—… ê³„íš:")
        for i, task in enumerate(task_list, 1):
            print(f"  {i}. {task[0]}#{task[1]}, {task[2]}")
        
        print(f"\nâœ… í”Œëœ ID: {plan_id[:8]}...")
        
    except Exception as e:
        print(f"âŒ GlobalPlanner ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()